
######################### Loading and Augmenting Images #########################
######################### Start Training #########################
tensor(0.9315, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9961, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9249, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7679, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8729, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8502, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7955, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9144, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8730, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6967, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6857, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6399, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.0952, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6605, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6695, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6734, device='cuda:0', grad_fn=<NllLoss2DBackward>)
Progress:   0%|                                                                                                                           | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                      | 0/912 [00:00<?, ?it/s]

Training: (loss 1.0056):   3%|███▎                                                                                              | 31/912 [00:03<01:27, 10.03it/s]
tensor(0.6575, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9369, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7101, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6314, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7014, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6525, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9626, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7414, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7293, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6905, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5914, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6224, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5848, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6695, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8108, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.0056, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8397, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6535, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4962, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.5772):   6%|█████▋                                                                                            | 53/912 [00:05<01:24, 10.15it/s]
tensor(0.5200, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5797, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8507, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6439, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4890, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8187, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1524, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4288, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6489, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6284, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8344, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1294, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6858, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7616, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7351, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6990, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6267, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5772, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6345, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5115, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5579, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5988, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5570, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5323, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5642, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4435, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4616, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5296, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3948, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4380, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8046, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5143, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3909, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4684, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5701, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4913, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7411, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6587, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5767, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5492, device='cuda:0', grad_fn=<NllLoss2DBackward>)


Training: (loss 0.4255):  11%|██████████▍                                                                                       | 97/912 [00:09<01:16, 10.66it/s]
tensor(0.5745, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5665, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4992, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4895, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.2020, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9405, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5653, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6894, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4804, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4228, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5704, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6205, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7727, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1456, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7276, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5529, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1586, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5990, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5952, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4255, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.4504):  13%|████████████▋                                                                                    | 119/912 [00:11<01:10, 11.22it/s]
tensor(0.6658, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5515, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4770, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3985, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3842, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8065, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4696, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1923, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3985, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6500, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4152, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4861, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2970, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.1620, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4450, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5831, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6310, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4649, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4209, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5230, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4504, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.4953):  15%|██████████████▎                                                                                  | 135/912 [00:13<01:19,  9.81it/s]
tensor(0.5840, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5508, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5125, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4445, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4718, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4137, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8064, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3994, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2882, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5261, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4477, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5650, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4625, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5640, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5159, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4953, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4102, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5566, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7166, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.3032):  17%|████████████████▋                                                                                | 157/912 [00:15<01:11, 10.57it/s]
tensor(0.6792, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4007, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5981, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6726, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5741, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4115, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5618, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4292, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3579, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4772, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7069, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2749, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2815, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4284, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4319, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5611, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3032, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5388, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4689, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6502, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.3866):  19%|██████████████████▊                                                                              | 177/912 [00:17<01:12, 10.13it/s]
tensor(0.4044, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5447, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3997, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3202, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5479, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.0233, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5146, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3194, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5829, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4157, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5950, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6097, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6075, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4613, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3002, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7715, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3866, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4567, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4994, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5981, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4782, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.4722):  22%|█████████████████████▍                                                                           | 201/912 [00:19<01:08, 10.44it/s]
tensor(0.2744, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.3185, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3939, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4025, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4526, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2832, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3904, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3622, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4047, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4213, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2759, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5767, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4735, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(1.2522, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5356, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4148, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6170, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4722, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.8467, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6376, device='cuda:0', grad_fn=<NllLoss2DBackward>)

Training: (loss 0.4414):  24%|███████████████████████▍                                                                         | 220/912 [00:21<01:19,  8.68it/s]
tensor(0.4484, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4233, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3171, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6399, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5765, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.2869, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.9818, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.7907, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5286, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.6877, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5219, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3575, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.5395, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3763, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3991, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4969, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.4414, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3747, device='cuda:0', grad_fn=<NllLoss2DBackward>)
tensor(0.3842, device='cuda:0', grad_fn=<NllLoss2DBackward>)
Traceback (most recent call last):
  File "main.py", line 71, in <module>
    train_model(
  File "main.py", line 57, in train_model
    DeepSeg.train_model()
  File "/root/research/deep-segmentation/segmentation/unet/segmentation.py", line 238, in train_model
    training_losses, validation_losses, lr_rates = trainer.run_trainer()
  File "/root/research/deep-segmentation/segmentation/unet/trainer.py", line 58, in run_trainer
    self._train()
  File "/root/research/deep-segmentation/segmentation/unet/trainer.py", line 91, in _train
    loss.backward()  # one backward pass
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt